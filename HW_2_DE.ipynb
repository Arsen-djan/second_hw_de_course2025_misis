{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19efae20",
   "metadata": {
    "id": "19efae20"
   },
   "source": [
    "**1. Смотрим на Hadoop Distributed File System**\n",
    "\n",
    "В рамках этой части вам нужно будет обращаться к HDFS с помощью CLI, разместить файлы для следующих заданий в распределеннй файловой системе и выполнить несколько преобразований над ними.\n",
    "\n",
    "Для работы файлы можно скачать по следующим ссылкам:\n",
    "- Логи посещения сайтов юзерами за некоторый промежуток времени [ссылка](https://drive.google.com/file/d/1WXyq5WVSWwJYXPuH4kyAJ5mrR3XgfO_H/view?usp=sharing)\n",
    "\n",
    "Разместите их в нашем внутреннем файловом хранилище с помощью HDFS CLI, для дальнейшего удобства под каждый файл стоит создать каталог с простым и понятным именем, разместить сами файлы в разных каталогах.\n",
    "\n",
    "Набор комманд, которые вам могут в этом помочь, доступны [здесь](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "\n",
    "В ячейках ниже должен быть полный набор комманд ваших обращей к консоли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c0f990",
   "metadata": {
    "id": "b4c0f990",
    "outputId": "4b0cd4da-bd10-4619-ac02-cda922c3fd64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwx------   - mapred hadoop          0 2025-04-12 10:28 /hadoop\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-12 10:28 /tmp\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-12 17:51 /user\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-12 10:28 /var\r\n"
     ]
    }
   ],
   "source": [
    "## вы можете обращаться к консоли из ноутбука таким способом\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a5a267",
   "metadata": {
    "id": "41a5a267",
    "outputId": "33b4bc5e-9527-4241-a541-83ae98ce3891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwx------   - mapred hadoop          0 2025-04-12 10:28 /hadoop\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-12 10:28 /tmp\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-12 17:51 /user\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-12 10:28 /var\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## или же использовать для этого меджик строчку в ячейке %%bash, как вам будет удобнее\n",
    "\n",
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61786cf4",
   "metadata": {
    "id": "61786cf4",
    "outputId": "51760388-8777-4b84-c564-d28e769ba12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `/input': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R -h /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6e16b",
   "metadata": {
    "id": "a0c6e16b"
   },
   "outputs": [],
   "source": [
    "## ваше решение здесь\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438ab221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-12 18:02 /user/hadoop\n",
      "drwxr-xr-x   - hive   hadoop          0 2025-04-12 10:28 /user/hive\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-14 19:36 /user/ubuntu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -ls /user/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6640914",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## загрузим файл bible.txt в hdfs\n",
    "hdfs dfs -test -d /user/ubuntu/bible || hdfs dfs -mkdir /user/ubuntu/bible\n",
    "hdfs dfs -test -f /user/ubuntu/bible/bible.txt || -put ./bible.txt /user/ubuntu/bible/bible.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac906e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## загрузим файл data_url.csv в hdfs\n",
    "hdfs dfs -test -d /user/ubuntu/data_url || hdfs dfs -mkdir /user/ubuntu/\n",
    "hdfs dfs -test -f /user/ubuntu/data_url/data_url.csv || -put ./data_url.csv /user/ubuntu/data_url/data_url.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63e2771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 ubuntu hadoop    4047392 2025-04-13 13:39 /user/ubuntu/bible/bible.txt\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 11:24 /user/ubuntu/bible/word_count_task\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "## проверим наличие файла bible.txt в указанном каталоге\n",
    "hdfs dfs -ls /user/ubuntu/bible/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38e7f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 13:21 /user/ubuntu/data_url/First_Step\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 13:29 /user/ubuntu/data_url/Second_Step\n",
      "-rw-r--r--   1 ubuntu hadoop   36443383 2025-04-13 13:42 /user/ubuntu/data_url/data_url.csv\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2025-04-15 13:19 /user/ubuntu/data_url/mapper2_error.log\n",
      "-rw-r--r--   1 ubuntu hadoop   10812091 2025-04-15 13:24 /user/ubuntu/data_url/sorted_and_agg\n",
      "-rw-r--r--   1 ubuntu hadoop       1490 2025-04-15 13:23 /user/ubuntu/data_url/top_finko_sites\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 12:12 /user/ubuntu/data_url/top_sites_per_day_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "## проверим наличие файла data_url.csv в указанном каталоге\n",
    "hdfs dfs -ls /user/ubuntu/data_url/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d84875",
   "metadata": {},
   "outputs": [],
   "source": [
    "## прочитаем часть каждого файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "145b75f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning God created the heaven and the earth. And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. \n",
      "And God said, Let there be light: and there was light. \n",
      "And God saw the light, that it was good: and God divided the light from the darkness. \n",
      "And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day. \n",
      "And God said, Let there be a firmament in the midst of the waters, and let it divide the waters from the waters. \n",
      "And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so. \n",
      "And God called the firmament Heaven. And the evening and the morning were the second day. \n",
      "And God said, Let the waters under the heaven be gathered together unto one place, and let the dry land appear: and it was so. \n",
      "And God called the dry land Earth; and the gathering together of the waters called he Seas: and God saw that it was good. \n",
      "And God said, Let the earth bring forth grass, the herb yielding seed, and the fruit tree yielding fruit after his kind, whose seed is in itself, upon the earth: and it was so. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -cat /user/ubuntu/bible/bible.txt | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5621a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gonzales-bautista.com/;2024-05-28 05:58:33.853479\r\n",
      "https://gonzales-bautista.com/;2024-05-26 03:44:36.853479\r\n",
      "https://www.davis-berg.com/;2024-05-31 17:22:01.853479\r\n",
      "https://www.montoya.com/;2024-05-29 13:41:35.853479\r\n",
      "https://gonzales-bautista.com/;2024-06-01 06:49:16.853479\r\n",
      "https://www.davis-berg.com/;2024-05-28 22:44:29.853479\r\n",
      "https://gonzales-bautista.com/;2024-05-27 23:48:30.853479\r\n",
      "https://gonzales-bautista.com/;2024-05-31 13:04:24.853479\r\n",
      "https://www.davis-berg.com/;2024-05-27 12:23:23.853479\r\n",
      "https://gonzales-bautista.com/;2024-05-27 16:52:14.853479\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -cat /user/ubuntu/data_url/data_url.csv | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f83ac",
   "metadata": {
    "id": "1e1f83ac"
   },
   "source": [
    "**2. Решаем задачи MapReduce**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990332ef",
   "metadata": {
    "id": "990332ef"
   },
   "source": [
    "**2.1 Подсчет слов в тексте**\n",
    "\n",
    "В рамках данного задания вам нужно подсчитать кол-во слов в тексте Библии (файл приложен к ДЗ в чате тг), то есть необходимо реализовать базовый функционал утилиты word count.\n",
    "\n",
    "**Важно** - подсчитывайте число только тех слов, длина которых больше 4 символов. Проводить процесс удаления знаков препинания и прочих символов **не нужно**\n",
    "\n",
    "Ниже вам представлены ячейки, в которых вы должны описать структуру маппера/редьсюера и ниже вызвать их в bash-скрипте запуска MR-таски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295c61fd",
   "metadata": {
    "id": "295c61fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "## скрипт для работы маппера\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    words = line.strip().split()\n",
    "    for word in words:\n",
    "        if len(word) > 4:\n",
    "            print(f\"{word.lower()}\\t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1bef060",
   "metadata": {
    "id": "c1bef060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "## скрипт для работы редьюсера\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, count = line.strip().split('\\t')\n",
    "    count = int(count)\n",
    "\n",
    "    if word == current_word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word is not None:\n",
    "            print(f'{current_word}\\t{current_count}')\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "\n",
    "# выводим ластовую пару\n",
    "if current_word is not None:\n",
    "    print(f'{current_word}\\t{current_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b35cd9",
   "metadata": {
    "id": "97b35cd9"
   },
   "source": [
    "В качестве проверки ваших python-скриптов до запуска MR таски можно произвести их запуск через консольные команды\n",
    "\n",
    "Тогда наша задача не будет выполняться через датаноды, а только на локальной машине, но в случае ошибок в скриптах вы увидите их и сможете исправить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5975e36",
   "metadata": {
    "id": "a5975e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall\t9733\n",
      "which\t4244\n",
      "their\t3859\n",
      "there\t2008\n",
      "before\t1722\n",
      "lord,\t1613\n",
      "against\t1596\n",
      "shalt\t1589\n",
      "children\t1560\n",
      "said,\t1556\n",
      "them,\t1549\n",
      "saying,\t1272\n",
      "house\t1222\n",
      "every\t1208\n",
      "people\t1194\n",
      "because\t1178\n",
      "thee,\t1170\n",
      "these\t1147\n",
      "saith\t1135\n",
      "after\t1128\n",
      "behold,\t1073\n",
      "therefore\t1054\n",
      "israel\t1025\n",
      "among\t907\n",
      "thine\t883\n",
      "neither\t861\n",
      "great\t847\n",
      "brought\t815\n",
      "things\t780\n",
      "jesus\t777\n",
      "should\t772\n",
      "according\t755\n",
      "israel,\t724\n",
      "forth\t720\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## пример запуска скриптов на неймноде для проверки их работы\n",
    "## добавил сортировку по убыванию количества слов для удобства \n",
    "\n",
    "cat bible.txt | python3 mapper.py | sort -k1,1 | python3 reducer.py | sort -k2,2nr | head -n 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279f6c1",
   "metadata": {
    "id": "8279f6c1"
   },
   "source": [
    "Как только в данной проверке вы получите успешный и корректный результат, можете запустить Map Reduce в ячейке ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ecb1749",
   "metadata": {
    "id": "1ecb1749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/ubuntu/bible/word_count_task\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob7557932364464668.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 11:23:59,284 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net/10.130.0.22:8032\n",
      "2025-04-15 11:23:59,519 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net/10.130.0.22:10200\n",
      "2025-04-15 11:23:59,560 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net/10.130.0.22:8032\n",
      "2025-04-15 11:23:59,561 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net/10.130.0.22:10200\n",
      "2025-04-15 11:23:59,773 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1744712451537_0006\n",
      "2025-04-15 11:24:00,110 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-15 11:24:00,218 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-15 11:24:00,392 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744712451537_0006\n",
      "2025-04-15 11:24:00,393 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-15 11:24:00,611 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-15 11:24:00,612 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-15 11:24:00,716 INFO impl.YarnClientImpl: Submitted application application_1744712451537_0006\n",
      "2025-04-15 11:24:00,773 INFO mapreduce.Job: The url to track the job: http://rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net:8088/proxy/application_1744712451537_0006/\n",
      "2025-04-15 11:24:00,775 INFO mapreduce.Job: Running job: job_1744712451537_0006\n",
      "2025-04-15 11:24:05,858 INFO mapreduce.Job: Job job_1744712451537_0006 running in uber mode : false\n",
      "2025-04-15 11:24:05,861 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-15 11:24:11,928 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2025-04-15 11:24:12,939 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2025-04-15 11:24:13,945 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2025-04-15 11:24:14,951 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2025-04-15 11:24:18,974 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-04-15 11:24:19,980 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-04-15 11:24:21,991 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2025-04-15 11:24:25,008 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "2025-04-15 11:24:26,013 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2025-04-15 11:24:28,023 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2025-04-15 11:24:30,037 INFO mapreduce.Job:  map 63% reduce 2%\n",
      "2025-04-15 11:24:31,042 INFO mapreduce.Job:  map 73% reduce 4%\n",
      "2025-04-15 11:24:32,049 INFO mapreduce.Job:  map 80% reduce 4%\n",
      "2025-04-15 11:24:35,065 INFO mapreduce.Job:  map 83% reduce 4%\n",
      "2025-04-15 11:24:36,070 INFO mapreduce.Job:  map 87% reduce 8%\n",
      "2025-04-15 11:24:37,078 INFO mapreduce.Job:  map 93% reduce 9%\n",
      "2025-04-15 11:24:38,087 INFO mapreduce.Job:  map 100% reduce 9%\n",
      "2025-04-15 11:24:39,092 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "2025-04-15 11:24:40,097 INFO mapreduce.Job:  map 100% reduce 58%\n",
      "2025-04-15 11:24:41,103 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2025-04-15 11:24:42,108 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "2025-04-15 11:24:43,112 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "2025-04-15 11:24:44,117 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-15 11:24:45,131 INFO mapreduce.Job: Job job_1744712451537_0006 completed successfully\n",
      "2025-04-15 11:24:45,211 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=170689\n",
      "\t\tFILE: Number of bytes written=10928182\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7737271\n",
      "\t\tHDFS: Number of bytes written=276444\n",
      "\t\tHDFS: Number of read operations=150\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=36\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tData-local map tasks=20\n",
      "\t\tRack-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=363405\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=351573\n",
      "\t\tTotal time spent by all map tasks (ms)=121135\n",
      "\t\tTotal time spent by all reduce tasks (ms)=117191\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=121135\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=117191\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=372126720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=360010752\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30383\n",
      "\t\tMap output records=266384\n",
      "\t\tMap output bytes=2548937\n",
      "\t\tMap output materialized bytes=558197\n",
      "\t\tInput split bytes=4170\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=25424\n",
      "\t\tReduce shuffle bytes=558197\n",
      "\t\tReduce input records=266384\n",
      "\t\tReduce output records=25424\n",
      "\t\tSpilled Records=532768\n",
      "\t\tShuffled Maps =360\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=360\n",
      "\t\tGC time elapsed (ms)=4715\n",
      "\t\tCPU time spent (ms)=44540\n",
      "\t\tPhysical memory (bytes) snapshot=12765872128\n",
      "\t\tVirtual memory (bytes) snapshot=182335320064\n",
      "\t\tTotal committed heap usage (bytes)=12655263744\n",
      "\t\tPeak Map Physical memory (bytes)=359047168\n",
      "\t\tPeak Map Virtual memory (bytes)=4345982976\n",
      "\t\tPeak Reduce Physical memory (bytes)=256716800\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4346032128\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7733101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=276444\n",
      "2025-04-15 11:24:45,211 INFO streaming.StreamJob: Output directory: /user/ubuntu/bible/word_count_task\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# обязательная чистка директории, куда будем складывать результат отрабоки mr\n",
    "hdfs dfs -rm -r /user/ubuntu/bible/word_count_task || true\n",
    "\n",
    "# запус mr таски с указанием пути до нужного jar\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"word-count\" \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -input /user/ubuntu/bible/bible.txt \\\n",
    "    -output /user/ubuntu/bible/word_count_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c0ff5",
   "metadata": {
    "id": "fe9c0ff5"
   },
   "source": [
    "Мониторить процесс работы таски можно на nodemanager по порту 8088 (уже прокинут в конфиге), там будет UI, в котором будет видно вашу запущенную задачу и её статус."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62cdb6",
   "metadata": {
    "id": "4f62cdb6"
   },
   "source": [
    "Результат работы скрипта должен выглядеть следующим образом (вывод тестовый):\n",
    "\n",
    "```bash\n",
    "word count\n",
    "abtr 6852\n",
    "btoad 4237\n",
    "stress 1932\n",
    "zen 1885\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c03742dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 items\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2025-04-13 09:49 /user/ubuntu/word_count_task/_SUCCESS\n",
      "-rw-r--r--   1 ubuntu hadoop      22742 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00000\n",
      "-rw-r--r--   1 ubuntu hadoop      24221 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00001\n",
      "-rw-r--r--   1 ubuntu hadoop      23364 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00002\n",
      "-rw-r--r--   1 ubuntu hadoop      23217 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00003\n",
      "-rw-r--r--   1 ubuntu hadoop      22730 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00004\n",
      "-rw-r--r--   1 ubuntu hadoop      22994 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00005\n",
      "-rw-r--r--   1 ubuntu hadoop      23112 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00006\n",
      "-rw-r--r--   1 ubuntu hadoop      23354 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00007\n",
      "-rw-r--r--   1 ubuntu hadoop      23683 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00008\n",
      "-rw-r--r--   1 ubuntu hadoop      22051 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00009\n",
      "-rw-r--r--   1 ubuntu hadoop      22530 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00010\n",
      "-rw-r--r--   1 ubuntu hadoop      22446 2025-04-13 09:49 /user/ubuntu/word_count_task/part-00011\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## проверяем наличие файлов\n",
    "\n",
    "hdfs dfs -ls /user/ubuntu/word_count_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "868c1f67",
   "metadata": {
    "id": "868c1f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lord.\t674\n",
      "pray.\t5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## запустите эту команду, чтобы вывести счетчик определенных слов, которые мы указали на grep\n",
    "## Это нам будет необходимо для визуального анализа результата работы вашего скрипта\n",
    "## в sort можете указать тот разделитель колонок, с которым у вас результат выплевывает редьюсер\n",
    "\n",
    "hdfs dfs -cat /user/ubuntu/word_count_task/* | grep -E 'lord\\.|god\\.|pray\\.' | sort -t$'\\t' -k2.2nr | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511d296",
   "metadata": {
    "id": "1511d296"
   },
   "source": [
    "**2.2 Решаем задачу поиска самых посещаемых сайтов**\n",
    "\n",
    "В данном задании нужно поработать с логом данных о посещении юзерами различных сайтов.\n",
    "Формат данных: `url;временная метка`. Вам нужно вывести топ 5 сайтов по посещаемости в каждую из дат, которая представлена в наших данных.\n",
    "\n",
    "Результат работы скрипта должен выглядеть следующим образом:\n",
    "\n",
    "```bash\n",
    "date        site                            count\n",
    "2024-05-25  https://gonzales-bautista.com/  987\n",
    "2024-05-25  https://smith.com/              654\n",
    "2024-05-25  https://www.smith.com/          321\n",
    "```\n",
    "\n",
    "**Рекомендации**\n",
    "\n",
    "1. Вам могу пригодиться дополнительные параметры mr таски, отвечающие за настройку шаффла, и правил сортировки ключей внутри него. Почитать о примерах их использования можно [здесь](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#More_Usage_Examples).\n",
    "\n",
    "2. Не рекомендуем использовать `\\t` в качестве символа разделителя для сложного ключа (потому что по дефолту таб используется для разделения колонок данных, и ключом в таком случае будет только первая колонка до таба). Если вы будете собирать сложный ключ для нужной вам сортировки данных, лучше всего будет использовать другие симловы, к примеру `+, =`.\n",
    "\n",
    "3. Возможно, у вас не получится решить данную задачу за одну mr таску, тогда вы просто описываете в решении скрипты ваших мапперов, редьюсеров под каждую из mr тасок, которые вам нужно запустить для получения нужного результата.\n",
    "\n",
    "**Важно** помнить, что любой маппер и редьюсер должен работать за O(1) памяти, и если вы будете создавать какой-то список, куда будете складывать какие-то данные, то он не должен быть размера O(n). Если такой момент в вашем решении будет, пожалуйста, поясните его текстово, что с вашими переменными все ок, и у них нет размера O(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe909b",
   "metadata": {
    "id": "0cfe909b"
   },
   "outputs": [],
   "source": [
    "## ваше решение здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e7be47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gonzales-bautista.com/;2024-05-28 05:58:33.853479\r\n",
      "https://gonzales-bautista.com/;2024-05-26 03:44:36.853479\r\n",
      "https://www.davis-berg.com/;2024-05-31 17:22:01.853479\r\n",
      "https://www.montoya.com/;2024-05-29 13:41:35.853479\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## смотрим на данные, которые расположены в csv-файле\n",
    "\n",
    "hdfs dfs -cat /user/ubuntu/data_url/data_url.csv | head -n 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e604fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## здесь создаём файл, куда будем складывать все багованные данные из csv-файла\n",
    "\n",
    "hdfs dfs -touchz /user/ubuntu/data_url/mapper2_error.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "382e7465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 13:21 /user/ubuntu/data_url/First_Step\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 13:29 /user/ubuntu/data_url/Second_Step\n",
      "-rw-r--r--   1 ubuntu hadoop   36443383 2025-04-13 13:42 /user/ubuntu/data_url/data_url.csv\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2025-04-15 13:50 /user/ubuntu/data_url/mapper2_error.log\n",
      "-rw-r--r--   1 ubuntu hadoop   10812091 2025-04-15 13:24 /user/ubuntu/data_url/sorted_and_agg\n",
      "-rw-r--r--   1 ubuntu hadoop       1490 2025-04-15 13:23 /user/ubuntu/data_url/top_finko_sites\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 12:12 /user/ubuntu/data_url/top_sites_per_day_output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -ls /user/ubuntu/data_url/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14db0d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "LOG_PATH_LOCAL = \"/tmp/mapper2_error.log\"\n",
    "LOG_PATH_HDFS = \"/user/ubuntu/data_url/mapper2_error.log\"\n",
    "\n",
    "def log_to_hdfs(message):\n",
    "    try:\n",
    "        with open(LOG_PATH_LOCAL, \"a\") as f:\n",
    "            f.write(message + \"\\n\")\n",
    "        subprocess.run([ \n",
    "            \"hdfs\", \"dfs\", \"-appendToFile\", \n",
    "            LOG_PATH_LOCAL, \n",
    "            LOG_PATH_HDFS \n",
    "        ], stderr=subprocess.DEVNULL)\n",
    "        open(LOG_PATH_LOCAL, \"w\").close()\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    for line in sys.stdin:\n",
    "        parts = line.strip().split(';')\n",
    "        site, timestamp = parts\n",
    "        try:\n",
    "            date = timestamp[:10] # Извлекаем только дату (формат YYYY-MM-DD)\n",
    "            print(f\"{date}+{site}\\t1\") \n",
    "        except Exception as e_inner:\n",
    "            log_to_hdfs(f\"Date error: {str(e_inner)} | line: {line.strip()}\") # Логируем ошибку, если не удается обработать строку\n",
    "except Exception as e_outer:\n",
    "    log_to_hdfs(f\"Log error: {str(e_outer)}\") # Логируем ошибку на более высоком уровне\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42e30dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26+http://abbott-brown.com/\t1\n",
      "2024-05-26+http://abbott.com/\t1\n",
      "2024-05-26+http://abbott.com/\t1\n",
      "2024-05-26+http://abbott.com/\t1\n",
      "2024-05-26+http://abbott.com/\t1\n",
      "2024-05-26+http://abbott.com/\t1\n",
      "2024-05-26+http://abbott.com/\t1\n",
      "2024-05-26+http://abbott-ellis.com/\t1\n",
      "2024-05-26+http://abbott-hall.com/\t1\n",
      "2024-05-26+http://abbott-holden.com/\t1\n",
      "2024-05-26+http://abbott.net/\t1\n",
      "2024-05-26+http://abbott.net/\t1\n",
      "2024-05-26+http://abbott.net/\t1\n",
      "2024-05-26+http://abbott.net/\t1\n",
      "2024-05-26+http://abbott.org/\t1\n",
      "2024-05-26+http://abbott.org/\t1\n",
      "2024-05-26+http://abbott.org/\t1\n",
      "2024-05-26+http://abbott.org/\t1\n",
      "2024-05-26+http://abbott.org/\t1\n",
      "2024-05-26+http://abbott-weaver.biz/\t1\n",
      "2024-05-26+http://acevedo.biz/\t1\n",
      "2024-05-26+http://acevedo.biz/\t1\n",
      "2024-05-26+http://acevedo-black.com/\t1\n",
      "2024-05-26+http://acevedo.com/\t1\n",
      "2024-05-26+http://acevedo.com/\t1\n",
      "2024-05-26+http://acevedo.com/\t1\n",
      "2024-05-26+http://acevedo.com/\t1\n",
      "2024-05-26+http://acevedo.com/\t1\n",
      "2024-05-26+http://acevedo-keith.biz/\t1\n",
      "2024-05-26+http://acevedo-keith.biz/\t1\n",
      "2024-05-26+http://acevedo.org/\t1\n",
      "2024-05-26+http://acevedo-smith.com/\t1\n",
      "2024-05-26+http://acosta-anderson.com/\t1\n",
      "2024-05-26+http://acosta.biz/\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## посмотрим на результат работы маппера\n",
    "\n",
    "cat data_url.csv | python3 mapper2.py | sort -k1,1 | head -n 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bac835d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "\n",
    "import sys\n",
    "\n",
    "current_date_url = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    date_url, count = line.strip().split('\\t')\n",
    "    count = int(count)\n",
    "        \n",
    "    if date_url == current_date_url:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_date_url is not None:\n",
    "            current_date, current_url = current_date_url.split('+')\n",
    "            print(f'{current_date}\\t{current_url}\\t{current_count}')\n",
    "        current_date_url = date_url\n",
    "        current_count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2ff22da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26\thttps://gonzales-bautista.com/\t335\n",
      "2024-05-26\thttp://smith.com/\t235\n",
      "2024-05-26\thttps://www.smith.com/\t221\n",
      "2024-05-26\thttps://smith.com/\t212\n",
      "2024-05-26\thttp://www.smith.com/\t212\n",
      "2024-05-26\thttps://johnson.com/\t185\n",
      "2024-05-26\thttp://www.johnson.com/\t181\n",
      "2024-05-26\thttps://williams.com/\t177\n",
      "2024-05-26\thttps://www.johnson.com/\t177\n",
      "2024-05-26\thttp://johnson.com/\t175\n",
      "2024-05-26\thttp://www.williams.com/\t148\n",
      "2024-05-26\thttp://brown.com/\t140\n",
      "2024-05-26\thttp://martinez.com/\t133\n",
      "2024-05-26\thttps://www.brown.com/\t127\n",
      "2024-05-26\thttps://www.williams.com/\t127\n",
      "2024-05-26\thttps://jones.com/\t125\n",
      "2024-05-26\thttp://www.jones.com/\t122\n",
      "2024-05-26\thttps://www.garcia.com/\t118\n",
      "2024-05-26\thttps://www.miller.com/\t115\n",
      "2024-05-26\thttps://www.jones.com/\t112\n",
      "2024-05-26\thttp://miller.com/\t109\n",
      "2024-05-26\thttp://williams.com/\t108\n",
      "2024-05-26\thttps://brown.com/\t104\n",
      "2024-05-26\thttps://miller.com/\t104\n",
      "2024-05-26\thttp://davis.com/\t101\n",
      "2024-05-26\thttp://jones.com/\t101\n",
      "2024-05-26\thttps://clark.com/\t98\n",
      "2024-05-26\thttp://www.davis.com/\t96\n",
      "2024-05-26\thttps://www.davis.com/\t93\n",
      "2024-05-26\thttps://www.taylor.com/\t93\n",
      "2024-05-26\thttp://www.brown.com/\t93\n",
      "2024-05-26\thttp://garcia.com/\t91\n",
      "2024-05-26\thttp://www.miller.com/\t87\n",
      "2024-05-26\thttps://thomas.com/\t84\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## пример запуска скриптов на неймноде для проверки их работы\n",
    "## добавил сортировку по убыванию количества для удобства \n",
    "\n",
    "cat data_url.csv | python3 mapper2.py | sort -k1,1 | python3 reducer2.py | sort -k1,1 -k3,3nr | head -n 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fbc47ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top_FINKO_sites_per_day.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_FINKO_sites_per_day.py\n",
    "\n",
    "## этот скрипт нужен для вывода топ-5 сайтов по дате\n",
    "import sys\n",
    "\n",
    "current_date = None\n",
    "top_counter = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    date, url, count = line.strip().split('\\t')\n",
    "    if current_date is None:\n",
    "        current_date = date\n",
    "    \n",
    "    if current_date == date and top_counter < 5:\n",
    "        top_counter += 1\n",
    "        print(f\"{date}\\t{url}\\t{count}\")\n",
    "    elif current_date != date:\n",
    "        current_date = date\n",
    "        top_counter = 1\n",
    "        print(f\"{date}\\t{url}\\t{count}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58379160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/ubuntu/data_url/First_Step\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob968267187019660034.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 13:58:50,797 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net/10.130.0.22:8032\n",
      "2025-04-15 13:58:51,037 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net/10.130.0.22:10200\n",
      "2025-04-15 13:58:51,077 INFO client.RMProxy: Connecting to ResourceManager at rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net/10.130.0.22:8032\n",
      "2025-04-15 13:58:51,078 INFO client.AHSProxy: Connecting to Application History server at rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net/10.130.0.22:10200\n",
      "2025-04-15 13:58:51,307 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1744723097344_0007\n",
      "2025-04-15 13:58:51,657 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-15 13:58:51,761 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-15 13:58:51,950 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744723097344_0007\n",
      "2025-04-15 13:58:51,952 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-15 13:58:52,228 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-15 13:58:52,228 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-15 13:58:52,325 INFO impl.YarnClientImpl: Submitted application application_1744723097344_0007\n",
      "2025-04-15 13:58:52,383 INFO mapreduce.Job: The url to track the job: http://rc1d-dataproc-m-d5igd34gni5t45gu.mdb.yandexcloud.net:8088/proxy/application_1744723097344_0007/\n",
      "2025-04-15 13:58:52,385 INFO mapreduce.Job: Running job: job_1744723097344_0007\n",
      "2025-04-15 13:58:57,474 INFO mapreduce.Job: Job job_1744723097344_0007 running in uber mode : false\n",
      "2025-04-15 13:58:57,475 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-15 13:59:04,546 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2025-04-15 13:59:06,562 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2025-04-15 13:59:08,572 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2025-04-15 13:59:09,578 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-04-15 13:59:11,587 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-04-15 13:59:12,593 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "2025-04-15 13:59:13,610 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "2025-04-15 13:59:15,619 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2025-04-15 13:59:17,630 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2025-04-15 13:59:19,640 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2025-04-15 13:59:20,649 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2025-04-15 13:59:21,654 INFO mapreduce.Job:  map 80% reduce 0%\n",
      "2025-04-15 13:59:22,660 INFO mapreduce.Job:  map 80% reduce 23%\n",
      "2025-04-15 13:59:23,665 INFO mapreduce.Job:  map 87% reduce 23%\n",
      "2025-04-15 13:59:24,672 INFO mapreduce.Job:  map 90% reduce 23%\n",
      "2025-04-15 13:59:26,682 INFO mapreduce.Job:  map 93% reduce 23%\n",
      "2025-04-15 13:59:27,686 INFO mapreduce.Job:  map 100% reduce 23%\n",
      "2025-04-15 13:59:28,692 INFO mapreduce.Job:  map 100% reduce 31%\n",
      "2025-04-15 13:59:30,704 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-15 13:59:30,711 INFO mapreduce.Job: Job job_1744723097344_0007 completed successfully\n",
      "2025-04-15 13:59:30,796 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1934223\n",
      "\t\tFILE: Number of bytes written=13285144\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=39222452\n",
      "\t\tHDFS: Number of bytes written=10812091\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=16\n",
      "\t\tRack-local map tasks=14\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=373272\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=64464\n",
      "\t\tTotal time spent by all map tasks (ms)=124424\n",
      "\t\tTotal time spent by all reduce tasks (ms)=21488\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=124424\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=21488\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=382230528\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=66011136\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=700000\n",
      "\t\tMap output records=700000\n",
      "\t\tMap output bytes=25943383\n",
      "\t\tMap output materialized bytes=3827876\n",
      "\t\tInput split bytes=4350\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=274260\n",
      "\t\tReduce shuffle bytes=3827876\n",
      "\t\tReduce input records=700000\n",
      "\t\tReduce output records=274259\n",
      "\t\tSpilled Records=1400000\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=3502\n",
      "\t\tCPU time spent (ms)=45510\n",
      "\t\tPhysical memory (bytes) snapshot=10096283648\n",
      "\t\tVirtual memory (bytes) snapshot=134567485440\n",
      "\t\tTotal committed heap usage (bytes)=9828302848\n",
      "\t\tPeak Map Physical memory (bytes)=361742336\n",
      "\t\tPeak Map Virtual memory (bytes)=4345466880\n",
      "\t\tPeak Reduce Physical memory (bytes)=238190592\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4344901632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=39218102\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10812091\n",
      "2025-04-15 13:59:30,796 INFO streaming.StreamJob: Output directory: /user/ubuntu/data_url/First_Step\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "## запускаем MR-task\n",
    "hdfs dfs -rm -r /user/ubuntu/data_url/First_Step || true\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"First_job\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -files mapper2.py,reducer2.py \\\n",
    "    -mapper \"python3 mapper2.py\" \\\n",
    "    -reducer \"python3 reducer2.py\" \\\n",
    "    -input /user/ubuntu/data_url/data_url.csv \\\n",
    "    -output /user/ubuntu/data_url/First_Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36a4a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## отсортируем полученный в результате MR-task файл и применим к нему скрипт для определения топ-5\n",
    "hdfs dfs -cat /user/ubuntu/data_url/First_Step/part-00000 | sort -k1,1 -k3,3nr | \\\n",
    "python3 top_FINKO_sites_per_day.py | \\\n",
    "hdfs dfs -put -f - /user/ubuntu/data_url/top_finko_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a32c610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26\thttps://gonzales-bautista.com/\t335\n",
      "2024-05-26\thttp://smith.com/\t235\n",
      "2024-05-26\thttps://www.smith.com/\t221\n",
      "2024-05-26\thttps://smith.com/\t212\n",
      "2024-05-26\thttp://www.smith.com/\t212\n",
      "2024-05-27\thttps://gonzales-bautista.com/\t376\n",
      "2024-05-27\thttps://www.smith.com/\t270\n",
      "2024-05-27\thttps://smith.com/\t236\n",
      "2024-05-27\thttp://smith.com/\t215\n",
      "2024-05-27\thttp://www.smith.com/\t208\n",
      "2024-05-28\thttps://gonzales-bautista.com/\t368\n",
      "2024-05-28\thttps://smith.com/\t256\n",
      "2024-05-28\thttps://www.smith.com/\t251\n",
      "2024-05-28\thttp://smith.com/\t224\n",
      "2024-05-28\thttp://www.smith.com/\t204\n",
      "2024-05-29\thttps://gonzales-bautista.com/\t402\n",
      "2024-05-29\thttps://www.smith.com/\t242\n",
      "2024-05-29\thttp://www.smith.com/\t223\n",
      "2024-05-29\thttps://smith.com/\t220\n",
      "2024-05-29\thttp://smith.com/\t206\n",
      "2024-05-30\thttps://gonzales-bautista.com/\t353\n",
      "2024-05-30\thttps://smith.com/\t246\n",
      "2024-05-30\thttps://www.smith.com/\t239\n",
      "2024-05-30\thttp://smith.com/\t229\n",
      "2024-05-30\thttp://www.smith.com/\t225\n",
      "2024-05-31\thttps://gonzales-bautista.com/\t374\n",
      "2024-05-31\thttps://www.smith.com/\t244\n",
      "2024-05-31\thttp://smith.com/\t228\n",
      "2024-05-31\thttp://www.smith.com/\t221\n",
      "2024-05-31\thttps://smith.com/\t219\n",
      "2024-06-01\thttps://gonzales-bautista.com/\t379\n",
      "2024-06-01\thttps://www.smith.com/\t232\n",
      "2024-06-01\thttps://smith.com/\t226\n",
      "2024-06-01\thttps://johnson.com/\t195\n",
      "2024-06-01\thttp://smith.com/\t191\n",
      "2024-06-02\thttps://gonzales-bautista.com/\t7\n",
      "2024-06-02\thttp://smith.com/\t7\n",
      "2024-06-02\thttps://www.williams.com/\t6\n",
      "2024-06-02\thttp://lee.com/\t5\n",
      "2024-06-02\thttp://miller.com/\t5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "## выводим тот самый топ-5 по дням\n",
    "hdfs dfs -cat /user/ubuntu/data_url/top_finko_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ffeefcb",
   "metadata": {
    "id": "2ffeefcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28  https://gonzales-bautista.com/  368\n",
      "2024-05-28  https://smith.com/              256\n",
      "2024-05-28  https://www.smith.com/          251\n",
      "2024-05-28  http://smith.com/               224\n",
      "2024-05-28  http://www.smith.com/           204\n",
      "2024-05-30  https://gonzales-bautista.com/  353\n",
      "2024-05-30  https://smith.com/              246\n",
      "2024-05-30  https://www.smith.com/          239\n",
      "2024-05-30  http://smith.com/               229\n",
      "2024-05-30  http://www.smith.com/           225\n",
      "2024-06-02  https://gonzales-bautista.com/  7\n",
      "2024-06-02  http://smith.com/               7\n",
      "2024-06-02  https://www.williams.com/       6\n",
      "2024-06-02  http://lee.com/                 5\n",
      "2024-06-02  http://miller.com/              5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## запустите эту команду, чтобы вывести результат работы по определенным компаниям, которые мы указали на grep\n",
    "## Это нам будет необходимо для визуального анализа результата работы вашего скрипта\n",
    "## в sort можете указать тот разделитель колонок, с которым у вас результат выплевывает редьюсер\n",
    "## укажите путь до той директории на hdfs, куда вы складывали результат\n",
    "\n",
    "hdfs dfs -cat /user/ubuntu/data_url/top_finko_sites | grep -E '2024-05-28|2024-06-02|2024-05-30' | column -t -s$'\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90c07abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "## проверим, попало ли что-то в логи после операции map\n",
    "hdfs dfs -cat /user/ubuntu/data_url/mapper2_error.log"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
